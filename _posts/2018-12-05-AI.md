---
layout  : page
title   : "AI"
date    : 2018-10-26
author  : "pshung"
---
## 2018-12-06
[Grouped convolution](https://blog.yani.io/filter-group-tutorial/)
Divide number of channel into N groups, perform convoltuion on individual group, then concantnate the result.  

## 2018-12-07
[8-Bit Quantization and TensorFlow Lite: Speeding up mobile inference with low precision](https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd)

paper: Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks   
paper: Eyeriss v2: A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks    
# Batch Normalization:
adjust the input value of activation function so that the input values can scatter averagely rather than scatter in the saturation region. (e.g. tanh: -1 to 1)

# 2018-12-17
[Halide language] (https://scitechdaily.com/halide-a-new-and-improved-programming-language-for-image-processing-software/)
A new programming language for image processing. The key to the language is to separate the algorithm from the optimization (also called schedule). Let Halide compiler generates the optimized code instead of the hand-written optimized code which is usually unreadable and unmodifiable.  

[Fast Image Processing using Halide](https://www.highperformancegraphics.org/wp-content/uploads/2017/Special-Session/HPG2017_FastImageProcessing.pdf)
[Halide talk](https://www.youtube.com/watch?v=3uiEyEKji0M)
*operation fusion to improve the cache locality, but fusion is a tradeoff among locality, recomputation, and pallelism.

# 2018-12-18
*Relay: version 2.0 of NNVM in TVM, a high-level intermidiate representation.
[RCF](https://github.com/dmlc/tvm/issues/1673)
[Paper](https://dl.acm.org/citation.cfm?id=3211348)
[Introduction](https://docs.tvm.ai/dev/relay_intro.html)


#2018-12-20
[unofficial open source NVDLA compiler](https://github.com/icubecorp/nvdla_compiler)
*NVDLA doesn't have plan to release the source code of their compiler, but they will provide customizable compiler for user to add new hardward layers. [ref](https://github.com/nvdla/sw/issues/104)

*[Tutorial on Hardware Architectures for Deep Neural Networks](http://eyeriss.mit.edu/tutorial.html)

#2018-12-21
[DNN Accelerator Architecture â€“ SIMD or Systolic?](https://www.sigarch.org/dnn-accelerator-architecture-simd-or-systolic/)
* An Analytic Model for Cost-Benefit Analysis of Dataflows in DNN Accelerators