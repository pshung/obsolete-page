---
layout  : page
title   : "AI"
date    : 2018-10-26
author  : "pshung"
---
# 2019-03-20
Term: training set, validation set and test set. [link](https://blog.csdn.net/simongeek/article/details/78200127)
training set: 考古題.    
validation: 模擬考.    
test set: 真實考試.   

> A true measure of the performance of the network is to measure its performance on a data set not contained in the training data -- this is measured by the validation accuracy. If the train accuracy is high but the validation accuracy remains low, that means the network is overfitting and memorizing particular features in the training images that aren't helpful more generally. 
 
 
# 2019-03-19
Transfer learning example for image classification
[TensorFlow For Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0)  
[how to retrain images](https://www.tensorflow.org/hub/tutorials/image_retraining)    

[An interesting webside to play NN](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.58304&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&stepButton_hide=false)    

*[Launching TensorFlow Lite for Microcontrollers](https://petewarden.com/2019/03/07/launching-tensorflow-lite-for-microcontrollers/)
explain why model must be extremely small enough to fit in kilobytes of memory in MCU.
*[Tensorflow lite micro](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro)
more lightweight tensorflow designed for micro-level cpu. The core runtime takes only 16kB.   

# 2019-03-14

* Why tf-slim, tf-learn, keras, tensorlayer? They are high level API of tensorflow since tensorflow API is not user-friendly. [link](https://www.zhihu.com/question/50030898/answer/119785042)



* Weight regularization
A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more "regular". This is called "weight regularization", and it is done by adding to the loss function of the network a cost associated with having large weights.
L1 normalization: where the cost added is proportional to the absolute value of the weights coefficients 
L2 normalization: where the cost added is proportional to the square value of the weights coefficients.
以L2 正規化來解釋, 就是當這個weight 平方越大, 則增加penalty 到loss function. 且這個penalty 只在training 時加入.

* Dropout
randomly "dropping out" (i.e. set to zero) a number of output features of the layer during training

* Batch, Iteration, Epoch
Batch: the number of data set used to evalutate the loss function and update the parameters.    
Iteration: the number of batch to go over each data one time    
Epoch: the number of iteration to go over each data.   
For example:   
1000 pictures, batch 10.  It takes 10 pictures and updates the parameter.   
The process needs 1000/10 = 100 iteration. This is called one epoch.     
If epoch is two, the whole process will repeat so that each picture is viewed twice.    


## 2019-03-13
Seedbank: store AI examples written in colab.
Occam's razor: 在選擇兩個具有同樣解釋力的方法下, 選擇越少不必要假設的那個簡單方法往往是具有最大解釋能力的.
weight regularization. 

## 2019-03-11
[Google colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true)
An environment for developing AI. Using jupyter-like web environment.

[Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning](https://www.coursera.org/learn/introduction-tensorflow/home/welcome)
An online course to introduce Tensorflow.



## 2018-12-06
[Grouped convolution](https://blog.yani.io/filter-group-tutorial/)
Divide number of channel into N groups, perform convoltuion on individual group, then concantnate the result.  

## 2018-12-07
[8-Bit Quantization and TensorFlow Lite: Speeding up mobile inference with low precision](https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd)

paper: Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks   
paper: Eyeriss v2: A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks    
# Batch Normalization:
adjust the input value of activation function so that the input values can scatter averagely rather than scatter in the saturation region. (e.g. tanh: -1 to 1)

# 2018-12-17
[Halide language] (https://scitechdaily.com/halide-a-new-and-improved-programming-language-for-image-processing-software/)
A new programming language for image processing. The key to the language is to separate the algorithm from the optimization (also called schedule). Let Halide compiler generates the optimized code instead of the hand-written optimized code which is usually unreadable and unmodifiable.  

[Fast Image Processing using Halide](https://www.highperformancegraphics.org/wp-content/uploads/2017/Special-Session/HPG2017_FastImageProcessing.pdf)
[Halide talk](https://www.youtube.com/watch?v=3uiEyEKji0M)
*operation fusion to improve the cache locality, but fusion is a tradeoff among locality, recomputation, and pallelism.

# 2018-12-18
*Relay: version 2.0 of NNVM in TVM, a high-level intermidiate representation.
[RCF](https://github.com/dmlc/tvm/issues/1673)
[Paper](https://dl.acm.org/citation.cfm?id=3211348)
[Introduction](https://docs.tvm.ai/dev/relay_intro.html)


# 2018-12-20
[unofficial open source NVDLA compiler](https://github.com/icubecorp/nvdla_compiler)
*NVDLA doesn't have plan to release the source code of their compiler, but they will provide customizable compiler for user to add new hardward layers. [ref](https://github.com/nvdla/sw/issues/104)

*[Tutorial on Hardware Architectures for Deep Neural Networks](http://eyeriss.mit.edu/tutorial.html)

# 2018-12-21
[DNN Accelerator Architecture – SIMD or Systolic?](https://www.sigarch.org/dnn-accelerator-architecture-simd-or-systolic/)
* An Analytic Model for Cost-Benefit Analysis of Dataflows in DNN Accelerators

# 2019-01-11
Install anaconda to control package management and create a virtual environment. (install tensorflow)
downalod an appropriate anaconda
```bash
bash Anaconda2-2018.12-Linux-x86_64.sh
conda create -n tensorflow  # create a virtual environment
source activate tensorflow # activate it.
conda install numpy
conda install tensorflow
conda install pytorch-nightly-cpu -c pytorch  # install caffe2 without GPU (cpu only)
```
[reference](https://medium.com/@margaretmz/anaconda-jupyter-notebook-tensorflow-and-keras-b91f381405f8)

enable jupyter auto-complete
```bash
#generate a config file
ipython profile create
#edit the config file
c.Completer.greedy = True
```

connect to remote jupyter notebook
remote:
```
jupyter notebook --no-browser --port=8889
```
local:    
1.Using putty to create a ssh connect with tunnel enabled. Set source port  as 8888, set destination as localhost:8889, "ADD the setting", then connent and login.  
2.creat a brower to connet localhost:8888  
3.input the token shown in the remote connection  

## trouble shooting
from caffe2 import workspace, it shows an undefined symbol of protobuf. It's a version issue.    
```
conda list
conda remove libprotobuf
```

